{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etyj8FlUFvBQ"
      },
      "source": [
        "## ***Video to Audio conversion***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ut9BiUCMEJc"
      },
      "source": [
        "\n",
        "!pip install ffmpeg-python "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-ThuXBrurii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a182b7b-f4b6-4e51-fb71-00b959b7b6c1"
      },
      "source": [
        "!pip install pydub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgq-3nUyX3UP"
      },
      "source": [
        "!pip install moviepy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdU-iCjLYLxP"
      },
      "source": [
        "from moviepy.editor import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vVc5RbHYfXO"
      },
      "source": [
        "mp4_file =\"TY_PA_Nikita_Kokate_T208003.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUrkz3AiY-Ic"
      },
      "source": [
        "mp3_file = \"audio1.mp3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkp1FBJkZP6D"
      },
      "source": [
        "videoClip = VideoFileClip(mp4_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40_oWyX3ZaSY"
      },
      "source": [
        "audioclip = videoClip.audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu3Q_kuYDYtp",
        "outputId": "b0ecc280-3c56-4e52-eba0-0ce4c3a3d3f5"
      },
      "source": [
        "audioclip.write_audiofile(mp3_file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[MoviePy] Writing audio in audio1.mp3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10783/10783 [00:13<00:00, 826.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[MoviePy] Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z16Wp3kKDkYV"
      },
      "source": [
        "audioclip.close()\n",
        "videoClip.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-7lhAONe9s4"
      },
      "source": [
        "## ***Audio to text conversion***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A28ZlElfNd4",
        "outputId": "40d6a13c-7d0a-42be-f92b-58dac9bb8b63"
      },
      "source": [
        "!pip3 install SpeechRecognition pydub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SpeechRecognition\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 118kB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl\n",
            "Installing collected packages: SpeechRecognition, pydub\n",
            "Successfully installed SpeechRecognition-3.8.1 pydub-0.25.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBLD34pIfbls"
      },
      "source": [
        "import speech_recognition as sr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P7V_EIlGaMX",
        "outputId": "69a69566-8989-4510-8730-fba1e14f1ab9"
      },
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "src = \"audio.mp3\"\n",
        "dst = \"audio.wav\"\n",
        "\n",
        "sound = AudioSegment.from_mp3(src)\n",
        "sound.export(dst, format=\"wav\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='audio.wav'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWqFF_vYfn9g"
      },
      "source": [
        "filename = \"machine-learning_speech-recognition_7601-291468-0006.wav\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB55KDoqE4Lr"
      },
      "source": [
        "filename = \"audio.wav\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BojKjMt0fy-A"
      },
      "source": [
        "# initialize the recognizer\n",
        "r = sr.Recognizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuTjgRwn9XYE"
      },
      "source": [
        "# open the file\n",
        "#with sr.Microphone() as source:\n",
        "   # print(\"Speak anything :\")\n",
        "    # listen for the data (load audio to memory)\n",
        "  #  audio = r.listen(source)\n",
        "    # recognize (convert from speech to text)\n",
        "   # text = r.recognize_google(audio)\n",
        "    #print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "neRzO7Adf4sB",
        "outputId": "e0b260dc-cfaf-48e8-beaf-96435091b5cb"
      },
      "source": [
        "# open the file\n",
        "with sr.AudioFile(filename) as source:\n",
        "    # listen for the data (load audio to memory)\n",
        "    audio_data = r.record(source)\n",
        "    # recognize (convert from speech to text)\n",
        "    text = r.recognize_google(audio_data)\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# attempt to read the file as WAV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename_or_fileobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlittle_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# RIFF WAV is a little-endian format (most ``audioop`` operations assume that the frames are stored in little-endian form)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/wave.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(f, mode)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWave_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/wave.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitfp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/wave.py\u001b[0m in \u001b[0;36minitfp\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'RIFF'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file does not start with RIFF id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'WAVE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: file does not start with RIFF id",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;31m# attempt to read the file as AIFF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maifc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename_or_fileobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlittle_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# AIFF is a big-endian format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/aifc.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(f, mode)\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mAifc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/aifc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitfp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/aifc.py\u001b[0m in \u001b[0;36minitfp\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'FORM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file does not start with FORM id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mformdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: file does not start with FORM id",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maifc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maiff_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maifc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/aifc.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(f, mode)\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mAifc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/aifc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# assume it is an open file object already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitfp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/aifc.py\u001b[0m in \u001b[0;36minitfp\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'FORM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/chunk.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, align, bigendian, inclheader)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunkname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-0b7b7e23d743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# open the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudioFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# listen for the data (load audio to memory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maudio_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# recognize (convert from speech to text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maifc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maiff_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maifc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Audio file could not be read as PCM WAV, AIFF/AIFF-C, or Native FLAC; check if file is corrupted or in another format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlittle_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# AIFF is a big-endian format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnchannels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Audio must be mono or stereo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Audio file could not be read as PCM WAV, AIFF/AIFF-C, or Native FLAC; check if file is corrupted or in another format"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M12FZ30y3R28"
      },
      "source": [
        "pip install pyaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCGGdT4xBY2B"
      },
      "source": [
        "## ***Text to summary***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QB24RqjzNQT"
      },
      "source": [
        "text = \"\"\"Today various organizations, be it online shopping, government and private sector organizations, catering and tourism industry or other institutions that offer customer services are concerned about their customers and ask for feedback every single time we use their services. Consider the fact, that these companies may be receiving enormous amounts of user feedback every single day. And it would become quite tedious for the management to sit and analyze each of those.\n",
        "But, the technologies today have reached to an extent where they can do all the tasks of human beings. And the field which makes these things happen is Machine Learning. The machines have become capable of understanding human languages using Natural Language Processing. Today researches are being done in the field of text analytics.\n",
        "And one such application of text analytics and NLP is a Feedback Summarizer which helps in summarizing and shortening the text in the user feedback. This can be done an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.\n",
        "\n",
        "If you’re interested in Data Analytics, you will find learning about Natural Language Processing very useful. Python provides immense library support for NLP. We will be using NLTK – the Natural Language Toolkit. which will serve our purpose right.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "e_jWkGBFzWw0",
        "outputId": "3b415129-2135-49e5-df94-65a3638c12e2"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Today various organizations, be it online shopping, government and private sector organizations, catering and tourism industry or other institutions that offer customer services are concerned about their customers and ask for feedback every single time we use their services. Consider the fact, that these companies may be receiving enormous amounts of user feedback every single day. And it would become quite tedious for the management to sit and analyze each of those.\\nBut, the technologies today have reached to an extent where they can do all the tasks of human beings. And the field which makes these things happen is Machine Learning. The machines have become capable of understanding human languages using Natural Language Processing. Today researches are being done in the field of text analytics.\\nAnd one such application of text analytics and NLP is a Feedback Summarizer which helps in summarizing and shortening the text in the user feedback. This can be done an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.\\n\\nIf you’re interested in Data Analytics, you will find learning about Natural Language Processing very useful. Python provides immense library support for NLP. We will be using NLTK – the Natural Language Toolkit. which will serve our purpose right.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atZLS5B7_-wp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYqQZqhYsFxY"
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iong_w4usEoF"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvcXVh8-f4ng"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from heapq import  nlargest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sZLg5zjsrui"
      },
      "source": [
        "stopwords = list(STOP_WORDS)\n",
        "stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nSLV9eLtm5R"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbcsk9NWt5Rq"
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL0nF_ABt_2z"
      },
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JmSbd0not5B0",
        "outputId": "a38746ca-77cd-4e2b-b992-8520e69e6677"
      },
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9SgUslkt4-7"
      },
      "source": [
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords:\n",
        "    if word.text.lower() not in punctuation:\n",
        "      if word.text not in word_frequencies.keys():\n",
        "        word_frequencies[word.text] = 1\n",
        "      else:\n",
        "        word_frequencies[word.text] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Yt3sgubvDgg",
        "outputId": "62314b7e-d58a-476d-af76-e065ef84a132"
      },
      "source": [
        "word_frequencies"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Abode': 1.0,\n",
              " 'Bowery': 1.0,\n",
              " 'Dutch': 1.0,\n",
              " 'Ingenuity': 1.0,\n",
              " 'Street': 1.0,\n",
              " 'abounded': 1.0,\n",
              " 'astonished': 1.0,\n",
              " 'beholders': 1.0,\n",
              " 'called': 1.0,\n",
              " 'carts': 1.0,\n",
              " 'city': 1.0,\n",
              " 'confound': 1.0,\n",
              " 'contrivances': 1.0,\n",
              " 'country': 1.0,\n",
              " 'distance': 1.0,\n",
              " 'dutch': 1.0,\n",
              " 'fire': 1.0,\n",
              " 'fixed': 1.0,\n",
              " 'horse': 1.0,\n",
              " 'horses': 1.0,\n",
              " 'meat': 1.0,\n",
              " 'ovens': 1.0,\n",
              " 'patent': 1.0,\n",
              " 'proofs': 1.0,\n",
              " 'required': 1.0,\n",
              " 'roasted': 1.0,\n",
              " 'seat': 1.0,\n",
              " 'short': 1.0,\n",
              " 'smokejacks': 1.0,\n",
              " 'soon': 1.0,\n",
              " 'turned': 1.0,\n",
              " 'weathercocks': 1.0,\n",
              " 'went': 1.0,\n",
              " 'wind': 1.0,\n",
              " 'work': 1.0,\n",
              " 'wrongheaded': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "915MRnpavDdq",
        "outputId": "3e7d8926-d6a7-4442-df71-861b1e767b25"
      },
      "source": [
        "max_frequency = max(word_frequencies.values())\n",
        "max_frequency"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOIJv6WGvDY8"
      },
      "source": [
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/ max_frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kQalztVvDTp",
        "outputId": "bef234a7-e46f-4044-cf02-c8565c28faf0"
      },
      "source": [
        "print(word_frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Today': 0.4, 'organizations': 0.4, 'online': 0.2, 'shopping': 0.2, 'government': 0.2, 'private': 0.2, 'sector': 0.2, 'catering': 0.2, 'tourism': 0.2, 'industry': 0.2, 'institutions': 0.2, 'offer': 0.2, 'customer': 0.2, 'services': 0.4, 'concerned': 0.2, 'customers': 0.2, 'ask': 0.2, 'feedback': 0.6, 'single': 0.4, 'time': 0.2, 'use': 0.2, 'Consider': 0.2, 'fact': 0.2, 'companies': 0.2, 'receiving': 0.2, 'enormous': 0.2, 'amounts': 0.2, 'user': 0.4, 'day': 0.2, 'tedious': 0.2, 'management': 0.2, 'sit': 0.2, 'analyze': 0.2, 'technologies': 0.2, 'today': 0.2, 'reached': 0.2, 'extent': 0.2, 'tasks': 0.2, 'human': 0.4, 'beings': 0.2, 'field': 0.4, 'makes': 0.2, 'things': 0.2, 'happen': 0.2, 'Machine': 0.2, 'Learning': 0.2, 'machines': 0.2, 'capable': 0.2, 'understanding': 0.2, 'languages': 0.2, 'Natural': 0.6, 'Language': 0.6, 'Processing': 0.4, 'researches': 0.2, 'text': 1.0, 'analytics': 0.4, 'application': 0.2, 'NLP': 0.4, 'Feedback': 0.2, 'Summarizer': 0.2, 'helps': 0.2, 'summarizing': 0.2, 'shortening': 0.2, 'algorithm': 0.2, 'reduce': 0.2, 'bodies': 0.2, 'keeping': 0.2, 'original': 0.4, 'meaning': 0.2, 'giving': 0.2, 'great': 0.2, 'insight': 0.2, '\\n\\n': 0.2, 'interested': 0.2, 'Data': 0.2, 'Analytics': 0.2, 'find': 0.2, 'learning': 0.2, 'useful': 0.2, 'Python': 0.2, 'provides': 0.2, 'immense': 0.2, 'library': 0.2, 'support': 0.2, 'NLTK': 0.2, '–': 0.2, 'Toolkit': 0.2, 'serve': 0.2, 'purpose': 0.2, 'right': 0.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTL29R3qCUyR",
        "outputId": "a0e58b77-f795-4bd5-d640-3852a32a8a68"
      },
      "source": [
        "select_length = int(len(tokens)*0.3)\n",
        "select_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUFGS6GGCoaH"
      },
      "source": [
        "summary = nlargest(select_length, word_frequencies, key = word_frequencies.get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGjLdDDuCy39",
        "outputId": "a2573205-bb9b-41fb-ca71-054b0cf8d3d5"
      },
      "source": [
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['text', 'feedback', 'Natural', 'Language', 'Today', 'organizations', 'services', 'single', 'user', 'human', 'field', 'Processing', 'analytics', 'NLP', 'original', 'online', 'shopping', 'government', 'private', 'sector', 'catering', 'tourism', 'industry', 'institutions', 'offer', 'customer', 'concerned', 'customers', 'ask', 'time', 'use', 'Consider', 'fact', 'companies', 'receiving', 'enormous', 'amounts', 'day', 'tedious', 'management', 'sit', 'analyze', 'technologies', 'today', 'reached', 'extent', 'tasks', 'beings', 'makes', 'things', 'happen', 'Machine', 'Learning', 'machines', 'capable', 'understanding', 'languages', 'researches', 'application', 'Feedback', 'Summarizer', 'helps', 'summarizing', 'shortening', 'algorithm', 'reduce', 'bodies', 'keeping', 'meaning', 'giving', 'great']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0Vdr7gsvDPb",
        "outputId": "f2ad7584-6d9a-449c-ca50-d49277f135a0"
      },
      "source": [
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Today various organizations, be it online shopping, government and private sector organizations, catering and tourism industry or other institutions that offer customer services are concerned about their customers and ask for feedback every single time we use their services., Consider the fact, that these companies may be receiving enormous amounts of user feedback every single day., And it would become quite tedious for the management to sit and analyze each of those.\n",
            ", But, the technologies today have reached to an extent where they can do all the tasks of human beings., And the field which makes these things happen is Machine Learning., The machines have become capable of understanding human languages using Natural Language Processing., Today researches are being done in the field of text analytics.\n",
            ", And one such application of text analytics and NLP is a Feedback Summarizer which helps in summarizing and shortening the text in the user feedback., This can be done an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text., \n",
            "\n",
            "If you’re interested in Data Analytics, you will find learning about Natural Language Processing very useful., Python provides immense library support for NLP., We will be using NLTK – the Natural Language Toolkit. which will serve our purpose right.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x4o9GGsvDMl"
      },
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "  for word in sent:\n",
        "    if word.text.lower() in word_frequencies.keys():\n",
        "      if sent not in sentence_scores.keys():\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgmcIelZyIgM",
        "outputId": "f2248d48-1787-4980-8202-4edbcd16220a"
      },
      "source": [
        "sentence_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Today various organizations, be it online shopping, government and private sector organizations, catering and tourism industry or other institutions that offer customer services are concerned about their customers and ask for feedback every single time we use their services.: 6.000000000000002,\n",
              " Consider the fact, that these companies may be receiving enormous amounts of user feedback every single day.: 2.6,\n",
              " And it would become quite tedious for the management to sit and analyze each of those.: 0.8,\n",
              " But, the technologies today have reached to an extent where they can do all the tasks of human beings.: 1.5999999999999999,\n",
              " And the field which makes these things happen is Machine Learning.: 1.2,\n",
              " The machines have become capable of understanding human languages using Natural Language Processing.: 1.2,\n",
              " Today researches are being done in the field of text analytics.: 2.2,\n",
              " And one such application of text analytics and NLP is a Feedback Summarizer which helps in summarizing and shortening the text in the user feedback.: 4.800000000000001,\n",
              " This can be done an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.: 4.4,\n",
              " \n",
              " \n",
              " If you’re interested in Data Analytics, you will find learning about Natural Language Processing very useful.: 1.4,\n",
              " Python provides immense library support for NLP.: 0.8,\n",
              " We will be using NLTK – the Natural Language Toolkit. which will serve our purpose right.: 0.8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlVVhFrhycng",
        "outputId": "36d818f2-b686-4a01-8a9c-e2cbb0c8f7b4"
      },
      "source": [
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NUO4fxeyckU"
      },
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bQI5TigychM",
        "outputId": "887a2bf9-8de9-4041-a77d-758e51955705"
      },
      "source": [
        "summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Today various organizations, be it online shopping, government and private sector organizations, catering and tourism industry or other institutions that offer customer services are concerned about their customers and ask for feedback every single time we use their services.,\n",
              " And one such application of text analytics and NLP is a Feedback Summarizer which helps in summarizing and shortening the text in the user feedback.,\n",
              " This can be done an algorithm to reduce bodies of text but keeping its original meaning, or giving a great insight into the original text.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em7DFTm5gNi3"
      },
      "source": [
        "# importing libraries \n",
        "import speech_recognition as sr \n",
        "import os \n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "\n",
        "# create a speech recognition object\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# a function that splits the audio file into chunks\n",
        "# and applies speech recognition\n",
        "def get_large_audio_transcription(path):\n",
        "    \"\"\"\n",
        "    Splitting the large audio file into chunks\n",
        "    and apply speech recognition on each of these chunks\n",
        "    \"\"\"\n",
        "    # open the audio file using pydub\n",
        "    sound = AudioSegment.from_wav(path)  \n",
        "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
        "    chunks = split_on_silence(sound,\n",
        "        # experiment with this value for your target audio file\n",
        "        min_silence_len = 500,\n",
        "        # adjust this per requirement\n",
        "        silence_thresh = sound.dBFS-14,\n",
        "        # keep the silence for 1 second, adjustable as well\n",
        "        keep_silence=500,\n",
        "    )\n",
        "    folder_name = \"audio-chunks\"\n",
        "    # create a directory to store the audio chunks\n",
        "    if not os.path.isdir(folder_name):\n",
        "        os.mkdir(folder_name)\n",
        "    whole_text = \"\"\n",
        "    # process each chunk \n",
        "    for i, audio_chunk in enumerate(chunks, start=1):\n",
        "        # export audio chunk and save it in\n",
        "        # the `folder_name` directory.\n",
        "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
        "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
        "        # recognize the chunk\n",
        "        with sr.AudioFile(chunk_filename) as source:\n",
        "            audio_listened = r.record(source)\n",
        "            # try converting it to text\n",
        "            try:\n",
        "                text = r.recognize_google(audio_listened)\n",
        "            except sr.UnknownValueError as e:\n",
        "                print(\"Error:\", str(e))\n",
        "            else:\n",
        "                text = f\"{text.capitalize()}. \"\n",
        "                print(chunk_filename, \":\", text)\n",
        "                whole_text += text\n",
        "    # return the text for all chunks detected\n",
        "    return whole_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8OhqTcNgZYM",
        "outputId": "57142624-acb2-4b67-8751-ff8e29891091"
      },
      "source": [
        "path = \"machine-learning_speech-recognition_7601-291468-0006.wav\"\n",
        "print(\"\\nFull text:\", get_large_audio_transcription(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "audio-chunks/chunk1.wav : His abode which you had fixed in a bowery or country seat. \n",
            "audio-chunks/chunk2.wav : Have a short distance from the city. \n",
            "audio-chunks/chunk3.wav : Just at what is now called dutch street. \n",
            "audio-chunks/chunk4.wav : Sooner bounded with proofs of his ingenuity. \n",
            "audio-chunks/chunk5.wav : Patent smokejack. \n",
            "audio-chunks/chunk6.wav : It required a horse to work some. \n",
            "audio-chunks/chunk7.wav : Dutch oven roasted meat without fire. \n",
            "audio-chunks/chunk8.wav : Carts that went before the horses. \n",
            "audio-chunks/chunk9.wav : Weather cox that turned against the wind and other wrongheaded contrivances. \n",
            "audio-chunks/chunk10.wav : So just understand can found it all beholders. \n",
            "\n",
            "Full text: His abode which you had fixed in a bowery or country seat. Have a short distance from the city. Just at what is now called dutch street. Sooner bounded with proofs of his ingenuity. Patent smokejack. It required a horse to work some. Dutch oven roasted meat without fire. Carts that went before the horses. Weather cox that turned against the wind and other wrongheaded contrivances. So just understand can found it all beholders. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxlReh9EgZK7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}